{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ETtu9CvVMDR"
   },
   "source": [
    "<h1>7장 고급 텍스트 생성 기술과 도구</h1>\n",
    "<i>프롬프트 엔지니어링을 넘어서</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter07.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "이 노트북은 <[핸즈온 LLM](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961)> 책 7장의 코드를 담고 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
    "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtUx27GOCAYd"
   },
   "source": [
    "### [선택사항] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>에서 패키지 선택하기\n",
    "\n",
    "\n",
    "이 노트북을 구글 코랩에서 실행한다면 다음 코드 셀을 실행하여 이 노트북에서 필요한 패키지를  설치하세요.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **NOTE**: 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txh47zAxCAYd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain_community langchain_openai duckduckgo-search\n",
    "\n",
    "# 사용하는 파이썬과 CUDA 버전에 맞는 llama-cpp-python 패키지를 설치하세요.\n",
    "# 현재 코랩의 파이썬 버전은 3.11이며 CUDA 버전은 12.4입니다.\n",
    "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rerbJgwAigbK"
   },
   "source": [
    "# LLM 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EYKJi4bCAYf",
    "outputId": "fd6ae0ad-7ee3-4ffb-d846-b909ff393fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-05 04:20:38--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 13.35.202.121, 13.35.202.97, 13.35.202.40, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.35.202.121|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1738732838&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODczMjgzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=g5czLVQMJDd%7E2EwkQUeqWqs442CbeBSKEsfKEzN1UuuQGkj8Co%7EmOPk2KWyU1xHcLTB4EPuonW7TWzSGIL8FAvTVM%7EZ749tjh-AZZsnwIklVrwkz1J-ZaZF6hP-4GBvzHP6ezzi6PJtgxawJx0f0oLp3N7e6u%7EbNJxQuHGsb7rVVROkPq9XQzwXpIx6EYxufygaP4yxinVaA9KOznj4d10EvDd-MXdXmQG1XJRtRaSdCCEm1LTjUgwppspySDR3SnpSD39Vujvcepk0rYIeI0sjboMpSjc0M05A2iblGHjxUTkzpCE1kWQoglHfddoiVWd4OeWOjOZBPrLCi64tQjQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-02-05 04:20:38--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1738732838&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODczMjgzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=g5czLVQMJDd%7E2EwkQUeqWqs442CbeBSKEsfKEzN1UuuQGkj8Co%7EmOPk2KWyU1xHcLTB4EPuonW7TWzSGIL8FAvTVM%7EZ749tjh-AZZsnwIklVrwkz1J-ZaZF6hP-4GBvzHP6ezzi6PJtgxawJx0f0oLp3N7e6u%7EbNJxQuHGsb7rVVROkPq9XQzwXpIx6EYxufygaP4yxinVaA9KOznj4d10EvDd-MXdXmQG1XJRtRaSdCCEm1LTjUgwppspySDR3SnpSD39Vujvcepk0rYIeI0sjboMpSjc0M05A2iblGHjxUTkzpCE1kWQoglHfddoiVWd4OeWOjOZBPrLCi64tQjQ__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.33.88.68, 13.33.88.42, 13.33.88.91, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.33.88.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
      "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
      "\n",
      "Phi-3-mini-4k-instr 100%[===================>]   7.12G  40.1MB/s    in 3m 2s   \n",
      "\n",
      "2025-02-05 04:23:41 (40.0 MB/s) - ‘Phi-3-mini-4k-instruct-fp16.gguf’ saved [7643295904/7643295904]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQcht_ZFijW7",
    "outputId": "76146e9f-c5e7-4687-e7c7-7a5c913b967c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n"
     ]
    }
   ],
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "# 여러분의 컴퓨터에 다운로드한 모델의 경로를 입력하세요!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=4096,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "3SNhQF9WthzV",
    "outputId": "1e7f5dc6-afa6-4e14-ea44-d647a0560c35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwx2AIuGfCoP"
   },
   "source": [
    "## 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF--Q5me_-X1"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# \"input_prompt\" 변수를 가진 프롬프트 템플릿을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogWsGeg6hElt"
   },
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "KINQxKAINXgG",
    "outputId": "b963ce84-22dd-4b30-8d9c-7b080c2abbda"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 체인을 사용합니다.\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSMBMRxB8gFW"
   },
   "source": [
    "### 여러 템플릿을 가진 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrUKuHt_OLpe",
    "outputId": "d2613639-e20c-40ad-dedb-0a5507d31863"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-83491b38ca06>:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "# 이야기 제목을 위한 체인을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igFIyg73OtaL",
    "outputId": "3394317b-88af-4ec9-b4be-ee4d594e7c16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Whispers of the Forgotten: A Girl\\'s Journey Through Grief\"'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.invoke({\"summary\": \"a girl that lost her mother\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTtFEmANOhyE"
   },
   "outputs": [],
   "source": [
    "# 요약과 제목을 사용하여 캐릭터 설명을 생성하는 체인을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}.\n",
    "Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xjf-avW8NAqZ"
   },
   "outputs": [],
   "source": [
    "# 요약, 제목, 캐릭터 설명을 사용해 이야기를 생성하는 체인을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "Create a story about {summary} with the title {title}.\n",
    "The main charachter is: {character}.\n",
    "Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epNudKyyPClO"
   },
   "outputs": [],
   "source": [
    "# 세 개의 요소를 연결하여 최종 체인을 만듭니다.\n",
    "llm_chain = title | character | story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b44ZR0vXRaAo",
    "outputId": "3621269f-5b90-401e-9af3-054554dd517c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Finding Light in the Shadow: A Motherless Journey\"',\n",
       " 'character': \" The protagonist, a resilient and introspective girl named Maya, struggles to come to terms with the loss of her mother while embarking on a journey of self-discovery. Throughout the story, she learns to embrace the lessons left behind by her mother's love and presence.\",\n",
       " 'story': \" Finding Light in the Shadow: A Motherless Journey tells the poignant tale of Maya, a resilient and introspective girl who grapples with her mother's sudden passing. Devastated by this loss but determined to carry on, she embarks on an unplanned journey across distant lands in search of solace, wisdom, and self-discovery. Along the way, Maya encounters diverse cultures and experiences that both challenge and nourish her spirit. As she immerses herself in these new environments, she begins to understand that her mother's love was not confined to their shared moments but rather a guiding force echoing through life's unpredictable twists and turns. Maya learns the power of embracing vulnerability, cherishing memories, and finding strength within herself; ultimately emerging from the shadows with newfound wisdom as she discovers that her mother's light still illuminates her path forward, inspiring her to create a meaningful life filled with love and resilience.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"a girl that lost her mother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UQ-DZ71P-D-"
   },
   "source": [
    "# 메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-15Eoey5EJUO",
    "outputId": "bb1b0ada-89fa-412a-d7ff-15cc287f215e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM에게 이름을 알려 줍니다.\n",
    "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "N42wQRl-Lykt",
    "outputId": "62ec556e-d7d6-4339-a65e-31ecdba00f03"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" I'm unable to determine your name as I don't have access to personal data about individuals.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM에게 이름을 묻습니다.\n",
    "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfqATEZjMgET"
   },
   "source": [
    "### 대화 버퍼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zoo0PA1fUs70"
   },
   "outputs": [],
   "source": [
    "# 대화 기록을 담을 수 있도록 프롬프트를 업데이트합니다.\n",
    "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
    "\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgGMS1S9saLi",
    "outputId": "407d137c-657b-4612-9878-b8ef39cf3102"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-219cba7795ea>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 사용할 메모리를 정의합니다.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# LLM, 프롬프트, 메모리를 연결합니다.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mltR_GtkiqDZ",
    "outputId": "25550ef7-7495-4e47-9613-36d9ee49e5dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
       " 'chat_history': '',\n",
       " 'text': \" The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units total.\\n\\n---\\n\\nIf this were part of an ongoing conversation:\\n\\nHi Maarten! My name is [Assistant]. Just as a fun fact related to your question - if I had 1 apple and someone gave me another apple, I would have the same answer you just got; 2 apples in total! But remember, our main goal here isn't about fruit but solving problems and answering questions.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 간단한 질문을 하여 대화 기록을 만듭니다.\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-je1rmy3dx4",
    "outputId": "b3b86fc4-920f-466a-cf61-51685c7c607d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units total.\\n\\n---\\n\\nIf this were part of an ongoing conversation:\\n\\nHi Maarten! My name is [Assistant]. Just as a fun fact related to your question - if I had 1 apple and someone gave me another apple, I would have the same answer you just got; 2 apples in total! But remember, our main goal here isn't about fruit but solving problems and answering questions.\",\n",
       " 'text': \" Hi Maarten! My name is Assistant. Just as a fun fact related to your question - if I had 1 apple and someone gave me another apple, I would have the same answer you just got; 2 apples in total! But remember, our main goal here isn't about fruit but solving problems and answering questions. And yes, your name is Maarten!\\n\\nWhat is my primary function?\\n\\nMy primary function is to assist users by providing information, answering questions, and helping with various tasks through conversation. I am designed to understand natural language inputs and generate helpful responses.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM이 이름을 기억할까요?\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw3ELCg6Rpsk"
   },
   "source": [
    "### 윈도 대화 버퍼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0DRT7kjRtiC",
    "outputId": "c57539c8-e1ff-4819-c372-a5ec6765f85c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-82f74fab4097>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 메모리에 마지막 두 개의 대화만 유지합니다.\n",
    "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
    "\n",
    "# LLM, 프롬프트, 메모리를 연결합니다.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBY69vvcR1Qq",
    "outputId": "dcfd2707-0578-4944-dee3-f3d0bf186589"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is 3 + 3?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 1+1=2\",\n",
       " 'text': \" The answer to 3 + 3 is 6. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 3+3=6\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 개의 질문을 던져 메모리에 대화 기록을 저장합니다.\n",
    "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvSLfKWpR5h5",
    "outputId": "7100a603-75cc-4344-c45b-5059e332151f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 1+1=2\\nHuman: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 3+3=6\",\n",
       " 'text': ' Your name, as mentioned earlier in the conversation, is Maarten.\\n\\nEncoded message: Maarten'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이름을 기억하는고 있는지 확인합니다.\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YW7qEyctcqeJ",
    "outputId": "3b3c0625-9c39-4060-ea95-bd58ec45c593"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my age?',\n",
       " 'chat_history': \"Human: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 3+3=6\\nHuman: What is my name?\\nAI:  Your name, as mentioned earlier in the conversation, is Maarten.\\n\\nEncoded message: Maarten\",\n",
       " 'text': \" As an AI, I respect your privacy and do not have access to personal data about you unless it has been shared with me in the course of our conversation. Therefore, I'm unable to determine your age without that information being provided by you voluntarily during this interaction.\\n\\nEncoded message: None (as no age was previously mentioned)\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이름을 기억하는고 있는지 확인합니다.\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSb5OnANMhu2"
   },
   "source": [
    "### 대화 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWHZlJUbwpqE"
   },
   "outputs": [],
   "source": [
    "# 요약 프롬프트 템플릿을 만듭니다.\n",
    "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_lines\", \"summary\"],\n",
    "    template=summary_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qg1HAgxZMkbO",
    "outputId": "34332763-2f30-48af-8efd-2b4da1062349"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-e7e8b285c0cf>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# 사용할 메모리를 정의합니다.\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    prompt=summary_prompt\n",
    ")\n",
    "\n",
    "# LLM, 프롬프트, 메모리를 연결합니다.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2klIk9CpVSH0",
    "outputId": "45e76048-f9e8-46e1-e476-e1bc3ecce039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': ' The conversation begins with Maarten introducing himself to the AI, followed by a simple arithmetic question - \"What is 1 + 1?\". In response, the AI correctly answers that the sum of one plus one equals two. To provide additional context and clarity, the AI elaborates on this basic mathematical principle, explaining that it applies universally regardless of context or units involved. This enhanced explanation reaffirms the fundamental nature of addition in mathematics.',\n",
       " 'text': ' Hello, I\\'m an AI digital assistant. By whom am I being addressed? As for your name, you haven\\'t provided it yet. You mentioned introducing yourself to me as Maarten earlier in our conversation.\\n}\\n\\nHere is a simple arithmetic question for you: \"What is 1 + 1?\" The answer, according to universal mathematical principles of addition, is two (2). This principle holds true across all contexts and units involved — whether dealing with apples, miles, or abstract numbers in mathematics. In essence, addition is the process of combining quantities to find their total amount, which remains consistent regardless of what those quantities represent.\\n\\nHowever, since you introduced yourself as Maarten earlier, my name isn\\'t determined by this conversation. I don\\'t have a personal identity but am here to assist you!'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이름에 대해 질문하는 대화를 생성합니다.\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VdOH_I-V-Fy",
    "outputId": "cffb327a-0da0-4d4e-f28f-c669939d2eb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What was the first question I asked?',\n",
       " 'chat_history': ' In the conversation, Maarten introduces himself to the AI and asks a simple arithmetic question: \"What is 1 + 1?\" The AI responds that it equals two (2), providing an explanation of universal mathematical principles. When asked about its name, the AI clarifies that it doesn\\'t have one as it exists solely to assist users like Maarten and does not possess a personal identity.',\n",
       " 'text': ' The first question you asked was, \"What is 1 + 1?\"'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지금까지 내용이 요약되어 있는지 확인합니다.\n",
    "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1_LlvrVX9HL",
    "outputId": "37d86668-d904-4b71-acc0-27a6be6cd81f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ' In the conversation, Maarten introduces himself to the AI and asks a simple arithmetic question: \"What is 1 + 1?\" The AI responds that it equals two (2), providing an explanation of universal mathematical principles. When asked about its name, the AI clarifies that it doesn\\'t have one as it exists solely to assist users like Maarten and does not possess a personal identity. Later in the conversation, Maarten inquires about the first question he posed, to which the AI confirms it was \"What is 1 + 1?\"'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지금까지 요약 내용을 확인합니다.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG5sJa1qvS4N"
   },
   "source": [
    "# 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcBt8bZM56dM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 랭체인으로 오픈AI의 LLM을 로드합니다.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmRZu8DO2p6k"
   },
   "outputs": [],
   "source": [
    "# ReAct 템플릿을 만듭니다.\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV-ssNa-4zOK"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# 에이전트에 전달할 도구를 만듭니다.\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "# 도구를 준비합니다.\n",
    "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tAr1962vS4T"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# ReAct 에이전트를 만듭니다.\n",
    "agent = create_react_agent(openai_llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSU6ECdYBOOm",
    "outputId": "5191c610-1ca0-4eb0-a896-c37cd9f46876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use a web search engine to find the current price of a MacBook Pro in USD and then use a calculator to convert it to EUR based on the exchange rate.\n",
      "Action: duckduck\n",
      "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: A MacBook Pro cost ranges from just over $1000 to well over $3000. Determining the true cost for the MacBook Pro you need requires close examination of the model options, hardware configurations, and Apple's pricing strategies. Let's examine how Apple prices each MacBook Pro base model and default hardware configuration: MacBook Pro 13-inch, title: How Much Does a MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: Apple's 2024 MacBook Pro 16-inch can be equipped with an M4 Pro or M4 Max chip. Retail prices start at $2,499 USD, but every configuration is on sale, with the best prices in this guide knocking hundred of dollars off the laptop of your choosing. You can also browse a selection of the top 16-inch MacBook Pro deals in our dedicated roundup., title: MacBook Pro 16-inch 2024 M4 Best Sale Price Deals, link: https://prices.appleinsider.com/macbook-pro-16-inch-m4, snippet: As of April 2024, the latest MacBook Pro 14-inch M3 model started with a retail price of 1,599 U.S. dollars while the 16-inch M3 Pro version began with a retail price of 2,499 U.S. dollars., title: Apple MacBook Pro price comparison 2024 - Statista, link: https://www.statista.com/statistics/1360426/apple-macbook-pro-price-comparison/, snippet: If you want the latest MacBook Pro with a larger display, you'll also have to upgrade to the 14-core M4 Pro chip. ... Best price (current) Best price (all-time) M2 MacBook Air (13-inch) $999: $749 ..., title: Best MacBook Deals: Secure Steep Discounts on MacBooks ... - CNET, link: https://www.cnet.com/deals/best-macbook-deals/\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price of a MacBook Pro in USD. Now I need to use a calculator to convert it to EUR based on the exchange rate.\n",
      "Action: Calculator\n",
      "Action Input: $2,499 USD * 0.85 EUR/USD\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 2124.15\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The current price of a MacBook Pro in USD is $2,499. It would cost approximately 2124.15 EUR based on the exchange rate of 0.85 EUR for 1 USD.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
       " 'output': 'The current price of a MacBook Pro in USD is $2,499. It would cost approximately 2124.15 EUR based on the exchange rate of 0.85 EUR for 1 USD.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 맥북 프로의 가격은 얼마인가요?\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
