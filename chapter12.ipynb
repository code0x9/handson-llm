{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpBVeU0XX8Uk"
   },
   "source": [
    "<h1>12Ïû• ÏÉùÏÑ± Î™®Îç∏ ÎØ∏ÏÑ∏ ÌäúÎãùÌïòÍ∏∞</h1>\n",
    "<i>ÏÉùÏÑ± LLMÏùÑ ÎØ∏ÏÑ∏ ÌäúÎãùÌïòÍ∏∞ ÏúÑÌïú Îëê Îã®Í≥Ñ Ï†ëÍ∑º Î∞©ÏãùÏóê ÎåÄÌïú ÌÉêÌóò</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter12.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ <[Ìï∏Ï¶àÏò® LLM](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961)> Ï±Ö 11Ïû•Ïùò ÏΩîÎìúÎ•º Îã¥Í≥† ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
    "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGw_POflgIII"
   },
   "source": [
    "### [ÏÑ†ÌÉùÏÇ¨Ìï≠] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>ÏóêÏÑú Ìå®ÌÇ§ÏßÄ ÏÑ†ÌÉùÌïòÍ∏∞\n",
    "\n",
    "\n",
    "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÑ Íµ¨Í∏Ä ÏΩîÎû©ÏóêÏÑú Ïã§ÌñâÌïúÎã§Î©¥ Îã§Ïùå ÏΩîÎìú ÏÖÄÏùÑ Ïã§ÌñâÌïòÏó¨ Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏóêÏÑú ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄÎ•º  ÏÑ§ÏπòÌïòÏÑ∏Ïöî.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **NOTE**: Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùò ÏΩîÎìúÎ•º Ïã§ÌñâÌïòÎ†§Î©¥ GPUÎ•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§. Íµ¨Í∏Ä ÏΩîÎû©ÏóêÏÑúÎäî **Îü∞ÌÉÄÏûÑ > Îü∞ÌÉÄÏûÑ Ïú†Ìòï Î≥ÄÍ≤Ω > ÌïòÎìúÏõ®Ïñ¥ Í∞ÄÏÜçÍ∏∞ > T4 GPU**Î•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H9EuD4pvgIII"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5luSSUAu_6d"
   },
   "source": [
    "## ÏßÄÎèÑ ÌïôÏäµ ÎØ∏ÏÑ∏ ÌäúÎãù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPtcbw38_hVi"
   },
   "source": [
    "### Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593,
     "referenced_widgets": [
      "a5c758784a7c442882b0efa19caf8759",
      "6967313d7c6d4e978d1e2613d6e31742",
      "173317c17592425f9a093613fd91c48e",
      "60e7ad34f625415f890acbd6996324b7",
      "8746a955a8df4824b46b0c738ccd5dcb",
      "dea171bdb29841a2a2d970d3e770a24e",
      "52074b7d578c4d7eadb29533c01eed4e",
      "24d1f8affc0944849cc5a5415f103e54",
      "b2c4753a41814d6b9f20311d30a8bf6b",
      "4df87e0d76944a6e83f9fb4593cc7566",
      "d1503cb43c284387aacbd0cabe8f95f7",
      "34f2de9dc00847cb8c365357ab236f48",
      "a407c4fd33bf4bbe8707f85e481e7404",
      "c906017a45e9443f90a1640b0ac34fcb",
      "b41a8f62a2424a2da22d74e1bb74ed97",
      "63f0ff57932f4d8a9b0e43d7fd9d9486",
      "739c321ec645484b8abd60a744e0eb0a",
      "53e7dbd9f0e2476983876b38fcb17918",
      "f770233ef87b41efb551b41f641c4b91",
      "059767566e3e406086a8dc56a8b3105d",
      "6c64057197504f07bfa0f859cc1e0abf",
      "7de4e053bfac4a50ab971d7a77565e69",
      "552bae661a1c4f868f7078750f439742",
      "4360c39d693d40e090e4332222dcb43d",
      "a19c4956541c4175b66558b00a34c322",
      "d8a292d0a04d495d801ec23322852c21",
      "b69bd6bb77254e7f84f91ea73a53b18d",
      "d0b154bfdf5b4f76a7f86f0afd709421",
      "b6f0cc89b1474141b4201b00184ac7b6",
      "bb313d3f91af49539779c8abe3eae641",
      "1bf7765850424eab8f6fbf1990e7e869",
      "7fbf78797d9b49c286acb43c98744026",
      "eeec92b46aa649319358add8e10f48e0",
      "123b82f0c57848999f42c3026b83ce4c",
      "9f25ce9766934d20b6980de9d22611f9",
      "2909738305f346db85fe0754793efc15",
      "17434d5610d14b5ba72ac7ecae220071",
      "c1790c4337fa43fc86a009147be666c9",
      "1ac25a2f1c7e4433bb5decad63277e99",
      "204f106e33384afa99086cd081613e20",
      "858403e91f974ccfb7f5c74436177aa6",
      "944d1871d4a34472b8449e28db9520c1",
      "356fd64f8b4a4275b80bdea3700d5700",
      "db46730ce6784fce8b3bd44dd66160de",
      "9fe9190546394ff69872397f06368eb1",
      "ad16e6881b93416e9c1e73129490aaa6",
      "ebbb567db65d4e79ab18b97bc542aaa9",
      "c04564cbb11f44ca98ed4ca8757f3921",
      "15c7aeb58156457dacc216cc5032242f",
      "53a05f703bb1439e85730e6d8d4e5f83",
      "206a79599bec419b9d4b971c0a34c7d7",
      "059ab22871554b9c9de9a06994e0c6ef",
      "7915e97bfc2b4be78a0e8fd532cb15ad",
      "d3a4ec8881a64459b32b8390173d6e8c",
      "b0a10ed002934e1499f18053c3454aa2",
      "3bdb2a93cc344617a8eac306cfb9ce39",
      "0f228a966c28473ea4e3caf624580229",
      "4e8678c488ec42c69dd611955bd2f983",
      "fc975a2fb85e4e35a8f2684a638b517f",
      "cb4238153746490a96042c9aeda61039",
      "82175387196240ad946a12640738e73f",
      "2f978ffb5e6346eebcb3270e6d20049b",
      "07e3b33be3b74036b755706ea0c423db",
      "09b4da001c2f4629a4334418f2ad0eb4",
      "ba41572a0b534ee595dba01fffb3329c",
      "7995569085ec4f7ebe2d582e07d87db8",
      "58163aa28a8345158581f56632e8ca38",
      "d997374a65794777ad55f8de91540237",
      "c88fabf047854ffdb6832b150716cc4c",
      "92962c07aa854a48a39ffc32605d4b8e",
      "4e272ebd1bf243269980bb624b5fd3d1",
      "47da01b9c3294cb082b78c84b91607ae",
      "45c5f3e8de0e4c3dba219a35b12cf683",
      "138d997b87614fd7907080bea9e659cc",
      "8b49a5e6a1744952aea011bf18d55abf",
      "88ce46e55f074c1fb8f8e9adcf256c67",
      "35540fb1df4947ceaf8826aa6fd6d3d8",
      "8062347651d444af843dcc9010c70f15",
      "5b469a104d484abdba19b934df3b7fb0",
      "78ade5881332429983fccf6bdfc4e78a",
      "88494a056ac940a4993189a6c704c154",
      "c14b1e25e93840de998da52a005d646f",
      "031cd8d50d474a949fbdaea6e666cb5b",
      "9eb4af5fd07a44dcbd5c37967b3edef1",
      "e6c5c790cc9e4ad38d5b75faf05c963e",
      "582a076428dc4ebbb9111f70c70ea5d6",
      "3eaec2f12350464e8494a654f93b8e0c",
      "d66d0fac2dd34a97a81f8155213f0aae",
      "30b946cdf826422d97a77dc11d504534",
      "200a7cb8bf2249e3902e82fb3a974c57",
      "8ddd27fb4c5c4f4a8abdef9d5fba927c",
      "a0b59ae986034064944a02ca5119ddb3",
      "55984d33df7a41018c6a1ffad0aefcbf",
      "556a6dd90d0c44c6a74593fa6f7467ba",
      "008878eaebea4ee89d00ad72db77305d",
      "ac00bde68a314fe1afc75e1971322cdb",
      "153719cba9f04fab87af2bae60006cb8",
      "b8f7c9fc132345709b39f0f18dd6bafd",
      "0d44794adb5042c8963f8d4b5a5248c9",
      "fe3a9ac03f8d4f08aa2b11d8b1e5e660",
      "a1ce071fc6f8485d8b4d4e8d28b62187",
      "f7a7a79ca7134e3f999a0586dba0d703",
      "4c480576e7864eb983a6900ecfc57cdc",
      "f2b1f911aa1a49c8af3cb2fd0a0703b7",
      "872b68b918a04bfc8ac93b7f3ad655bd",
      "1238b5f1301f4810aedd0e2361374f8e",
      "b486a98eb2924f718acc2c647be64a3e",
      "e12deab9e2be42d7b599d58fecd62a71",
      "9ffb40a92f3442989ee95fffe3bc4a30",
      "ea2fc5bebdf7425a8df0cc4d22f9af3d",
      "9d5844a37a994a06a85e52d5d3e790ee",
      "90de6e1c15474beea8e4a817a8230513",
      "fd22d1efbafa4a59a0c01f6fc4031bcb",
      "ed9e3373af2a442f9dcf1b778981780f",
      "e39f54432d7249d58e20f1ac0a85d5c2",
      "e37aff39643d443698cb51e0f80352d4",
      "1b717353a7834da7a10eaa3786aad39a",
      "8fa213ee7d5f4d9a9fd736f6d8444e5e",
      "227e3660728f4d09a2f3aaf8cb4599f5",
      "ad9f2a7afad24a15b5a338c12068cfee",
      "6480c3cc5b48428c9ef62f07f47deb10",
      "164c38d444c04743b763452f2181c23f",
      "fd64113a0d1740fcab8fca5b82e1feeb",
      "945b1bb515734770aaf072482db5837d",
      "b112267bbb3c44ef87925f9d62dd1c88",
      "21ba072fadb744708e2f15b5745c1292",
      "8c88334132c6432aae9ef5143da61558",
      "577655bd61e74c5ebfbc444b1089619c",
      "26d66f66f63440f58ec01249b2299f97",
      "52c8d6a0289b4cea89686510a974afd0",
      "5228cdf5d45a4f09b46a6a37c285d52c",
      "749573e95b844a45a33a82a247454754",
      "5804610010f9417093ec7cfe42fef9dd",
      "c7246a28fbf24304a708d6562bb9c36a",
      "e6b9450a097c44e2b9c1f919cc1ec095",
      "2bdb7802d49141d794506da68cef8449",
      "7a1be28b7aa640f7b7deb2c010c8320f",
      "733e979ca44e4c95870a6d3f2cbc8e7e",
      "8e8f82388ca64de8b2bb2a1f73276e71",
      "f3567e98da0841c19cc3d1cb957060f7",
      "27c14fe88ad94a4086ff79344fc1a575",
      "22a49ebb20d64d69af7751630bd5c353",
      "2a0bec76ab394271a583ea7707323eab",
      "36d8d4d4a8c1493c83a7eb9955201e61",
      "5547bd71791344efb84c585ca9a0f7f3",
      "1543367bfe324067be1e35d35d05ab22",
      "672f71b7275044be9b4c2d22b1af5d69",
      "8b473a73365a476c848caf08e8ce6c39",
      "01d568dfb83c4cf69de88d6982f46228",
      "cbf8b73dd7a647028e443a3fdb031d50",
      "fe5b66046a1c41378d8f15579174db2b",
      "75ac20ad1a164778b5c5a3c43d648d66",
      "2a3a1a7264d449eb890f3df9ba8a058a",
      "dc628ccf7c6641b18edb5d052af6b1fc",
      "98d29464d918406abc23c44d5a8d8abc",
      "0974c4f821364f0cbda8fd5ad9be387b",
      "61b3aea803ad40c3977c154e52ad4a5f",
      "33682188b9ae42439d95b195a1fffc92",
      "1896d0f9cfd54e19bd3d52ea6e312b7e",
      "faf9e6a6e4be4e74bb610b9fec8a7dc4",
      "b9dd1af4df1a431fb4f8e4fe4b4ae27c",
      "8e2e12bd51f6425e90c8f0ece41e7ee5",
      "4b2f35244b00410e9c392e22bfd47b97",
      "daf534d6f9b64ed399b41fc2eaec7f11",
      "02dca6e5fb764558bf8234e51fe78032",
      "0d3e458858804ff29df8a27a18565a2e",
      "b11f09324807427482ddfca645cedbb8",
      "1c7884ad34194dbabb5c708d72c96d45",
      "14080f16bcd64963bc3150ac71f16b1e",
      "946ecd55905e4d7698f8f4ff5dfd61f2",
      "24b298321d7541b0805b3111143929b5",
      "48bd1d49062941d980c59389f5a5a0ab",
      "d33f66da7a0e4c8cb13ddba57c9ad76b",
      "88815e9c68094bc6a8d152b1e1ac62dd",
      "622393644ba341a094094734f7374754",
      "218eade411d44ef59763ee9824777c4a",
      "aca7bfbc548b4355830fe47176a75c47",
      "4f9a6cc485e243a6b2e53b99bd98633f",
      "8af5ae36a13b45cab14f50f9abd0a22a",
      "c7ffd69ee0cf4985bff166c744260bd7",
      "4c3fc7eea912428ca31a76fa5f720140",
      "fbe32f0d1d674b628db65071731386fa",
      "704a3b5f472645f18d034e33339e4daa",
      "80537d3c768441a48be15ac8bcb3a948",
      "06a1fb65818746a1aa03ad38f6cf3da8",
      "933f189baf524fa0b3461b9253e24377",
      "e6f257071c66447cbb86426e40db64ae",
      "f67b2f4b289c4f4ca5cbccd4374f37cb",
      "ba5e39afa97343c0a60ce59db57a1386",
      "509793821d2f44c786e787f698b89889",
      "e15f4c9926424254b141529f9a402cdd",
      "3eb7a0d6396643d095553d8584734c0f",
      "de7643d8fb6449cabc3ba29b71c90f2b",
      "3f37f398e6b2458a8c5f07f9ceed2b24",
      "d01aa99012084ef0a0508227d4006d10",
      "2d8467b0405b40f0b0e700027012382b",
      "c3aac064860c4734ba3ad53dbbc72fbf",
      "47d623b36fa24ba38db91c623899a295"
     ]
    },
    "id": "SqeZchJiOXdd",
    "outputId": "4a080d26-ac37-4794-ea5f-4eb7607f01e1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c758784a7c442882b0efa19caf8759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f2de9dc00847cb8c365357ab236f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552bae661a1c4f868f7078750f439742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123b82f0c57848999f42c3026b83ce4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe9190546394ff69872397f06368eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdb2a93cc344617a8eac306cfb9ce39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00003-a3ecf92756993583.parquet:   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58163aa28a8345158581f56632e8ca38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00001-of-00003-0a1804bcb6ae68c6.parquet:   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8062347651d444af843dcc9010c70f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00002-of-00003-ee46ed25cfae92c6.parquet:   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b946cdf826422d97a77dc11d504534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00001-f7dfac4afe5b93f4.parquet:   0%|          | 0.00/81.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3a9ac03f8d4f08aa2b11d8b1e5e660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00003-a6c9fb894be3e50b.parquet:   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5844a37a994a06a85e52d5d3e790ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00001-of-00003-d6a0402e417f35ca.parquet:   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164c38d444c04743b763452f2181c23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00002-of-00003-c0db75b92a2f48fd.parquet:   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5804610010f9417093ec7cfe42fef9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00001-3d4cd8309148a71f.parquet:   0%|          | 0.00/80.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d8d4d4a8c1493c83a7eb9955201e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_sft split:   0%|          | 0/207865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d29464d918406abc23c44d5a8d8abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_sft split:   0%|          | 0/23110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3e458858804ff29df8a27a18565a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_gen split:   0%|          | 0/256032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca7bfbc548b4355830fe47176a75c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_gen split:   0%|          | 0/28304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67b2f4b289c4f4ca5cbccd4374f37cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Ï±ÑÌåÖ ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º Î°úÎìúÌï©ÎãàÎã§.\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"TinyLlamaÏùò <|user|> ÌÖúÌîåÎ¶øÏúºÎ°ú ÌîÑÎ°¨ÌîÑÌä∏Î•º Ìè¨Îß∑ÌåÖÌï©ÎãàÎã§\"\"\"\n",
    "\n",
    "    # Ï±ÑÌåÖ ÌÖúÌîåÎ¶ø Íµ¨ÏÑ±\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌïòÍ≥† TinyLlama ÌÖúÌîåÎ¶øÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.\n",
    "dataset = (\n",
    "    load_dataset(\"HuggingFaceH4/ultrachat_200k\",  split=\"test_sft\")\n",
    "      .shuffle(seed=42)\n",
    "      .select(range(3_000))\n",
    ")\n",
    "dataset = dataset.map(format_prompt).remove_columns(['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtl2xZptgyDf",
    "outputId": "766b2e3a-a468-4196-c28b-99104ea43ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Given the text: Knock, knock. Who‚Äôs there? Hike.\n",
      "Can you continue the joke based on the given text material \"Knock, knock. Who‚Äôs there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Sure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s>\n",
      "<|user|>\n",
      "Can you tell me another knock-knock joke based on the same text material \"Knock, knock. Who's there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Of course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here and let's go for a walk!</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ÌîÑÎ°¨ÌîÑÌä∏ ÏòàÏãú\n",
    "print(dataset[\"text\"][2576])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyuLZGizDqUB"
   },
   "source": [
    "### Î™®Îç∏ ÏñëÏûêÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "fb67306d828e4e1cb6ea89aca75061c5",
      "17a4e1ce23e9473f9504cecd7e895952",
      "d48db3882c38488aa434ff63f779d62a",
      "fb39c88d0dd64553863285586f7d0473",
      "0085b83faf374ce5b3e829bfcbf1417d",
      "54e070ffe6d94f478242873d717f288c",
      "d478631c623e411cac9fb87e7dd8b83a",
      "6536c37d53b1459cac32852aec2c16bc",
      "ced8d519a42f49ceaa9f6c16c5fc5999",
      "b75363215eac444c8e40d650d375c03b",
      "ba3ea3503ab448e7a1866c411bb078d1",
      "540bb0409a614f1da52fd09304a72be7",
      "8f5439bca7654b11a01cdcb6141ddc60",
      "3e5f69eadcb146e3bc8b1aa95edebfc5",
      "0d3780c23ab145d48d562497cf380a81",
      "d42c2247ed9e4f3fa507c1985d70a04a",
      "fb881a8bdf5f4c6ea1add5dd85060865",
      "3677f38fbfa143c3883b2c7f562d3d91",
      "97c3a8bb63ec4b20b3f409a392708543",
      "44b95bc36d284758ac70f58a27407e87",
      "281fddeeec584e20a25673b529473b3d",
      "9efcb7c25a9c40479a71cd383cf52f23",
      "0c5621162289427aaf07ace5afaa9840",
      "9802bf2ee67f48f4b4065a0f0069ab6f",
      "4cbd7e5cfef744d9ba713d9f536b15ed",
      "e5f5db0797d54e8bbcc8835953cb2f65",
      "c0235e99bfb84267aa81b2dc3d4e347c",
      "1d52750d1bb84284b8b515d66534aca4",
      "2c7e2b584e2c4909bd2f2b9917cf7779",
      "ed87fa4a35cf4d7ca1d213e7d68630b4",
      "b58ca0b48cc1476ea8a735ed2b778ec4",
      "c2d38918e01d42df8b7de0d5e4accb56",
      "18711a7041a64412b5213b5f0be48004",
      "ac80402bbda8495c8c96c7a56acb00eb",
      "8728999831404715a81a5aff99909b85",
      "7e287d921d0746dca950fdfe5be604ab",
      "7a69f0f1ea23483ca6651b186766c3ac",
      "ee008ed7efd84c7c80da8929a130d844",
      "1bfb156f4f88440db122acca1fd7cc5d",
      "d8b5345653434814884137d14a643691",
      "fddbb45ecdc54f5a87ee811260420500",
      "5cfc740a47d7477ba28bbaebd3ce732a",
      "3b73373061f94fa6bc0490bb6c3b0ebb",
      "1091d31379a44f9db7971e06a5dc4751",
      "d0b445fd16f44601b5f0a8dda61b730d",
      "98a5d4352af44931bdc1de8fa1b232cd",
      "92b5d2a4fb2c4409945802aee40752fe",
      "b0232640416b44c9b8682530535ade7b",
      "64511479555146a68bfb40b6cf2496be",
      "ce0e5714803a4238b94447b8d3249473",
      "96fb9439a8ab4420ac17fed0fdd9c6a0",
      "dc08d4c4973b4edc8c8ae2770568b4e8",
      "4d297e8ef1a64b02883cf30319855f9f",
      "194e4d11ebdc44eda194af72848374e5",
      "a3116b3eb2c34847bbea545303adb33f",
      "e4265083d51342c9aa01e3b979d0995e",
      "031e9a5e4c48424f89270945508de145",
      "bf6c5fe94691438e9ce7d8126707a2bc",
      "50f4b37b069c4834a4f1c1b1c2a1579e",
      "7a52c17283db4de98eef0a87585badbf",
      "303a8a61bd604e498dbb240e1b2ef35f",
      "731aa3404c51409eb7ce5e2954846750",
      "7c01bc2bdb764e73ad7b5aacc07aac63",
      "91ac04cb47dd478e9ca7f205d75a952e",
      "6cb31ed8085b48f79afb051be37fd87a",
      "b7051578dbfb458d9df017d10bfb71db",
      "2759bb29b6d947108dccf1a885334a14",
      "7879395c5a23489f9ce5767aa9427205",
      "e679b40fa8a94b328ca3660395021349",
      "70b86bcd4a034784b243f9e03d97a0c6",
      "614be568acfd472487f8cf47e9e899b7",
      "176a7d4529c44c3eb29cf1a0128fdd21",
      "23ea49b98ce84a9c89a19c57d5d70c19",
      "6bbbe1b0ae764d599cdb3110b5df5f6e",
      "3005eb888dcd4fa0b2010dc4b0223bb7",
      "e1927414df4a4022b04497452e1f6804",
      "3902042e45a94436b0fcf62aaef0941e"
     ]
    },
    "id": "M95Y207T7wSp",
    "outputId": "1d5ef4ca-c9ea-436d-cfb7-db2bc5fd74cd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb67306d828e4e1cb6ea89aca75061c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540bb0409a614f1da52fd09304a72be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5621162289427aaf07ace5afaa9840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac80402bbda8495c8c96c7a56acb00eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b445fd16f44601b5f0a8dda61b730d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4265083d51342c9aa01e3b979d0995e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2759bb29b6d947108dccf1a885334a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-ÎπÑÌä∏ ÏñëÏûêÌôî ÏÑ§Ï†ï - QLoRAÏùò Q Îã®Í≥Ñ\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-ÎπÑÌä∏ Ï†ïÎ∞ÄÎèÑ Î™®Îç∏ Î°úÎìú\n",
    "    bnb_4bit_quant_type=\"nf4\",  # ÏñëÏûêÌôî Ï¢ÖÎ•ò\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Í≥ÑÏÇ∞ dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Ïù¥Ï§ë ÏñëÏûêÌôî Ï†ÅÏö©\n",
    ")\n",
    "\n",
    "# Î™®Îç∏ÏùÑ Î°úÎìúÌïòÍ≥† GPUÏóêÏÑú ÌõàÎ†®Ìï©ÎãàÎã§.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    # ÏùºÎ∞òÏ†ÅÏù∏ SFTÏóêÏÑúÎäî Îã§ÏùåÏùÑ ÏÇ≠Ï†úÌïòÏÑ∏Ïöî.\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# LLaMA ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1iGIch-sAMC"
   },
   "source": [
    "### ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86o1T5n4DziD"
   },
   "source": [
    "#### LoRA ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0tYs1ZhYDyw9"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA ÏÑ§Ï†ï\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Ïä§ÏºÄÏùºÎßÅ\n",
    "    lora_dropout=0.1,  # LoRA Ï∏µÏùò ÎìúÎ°≠ÏïÑÏõÉ\n",
    "    r=16,  # Îû≠ÌÅ¨\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # ÎåÄÏÉÅ Ï∏µ\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# ÌõàÎ†®ÏùÑ ÏúÑÌïú Î™®Îç∏ Ï§ÄÎπÑ\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhbh7kKuD24o"
   },
   "source": [
    "#### ÌõàÎ†® ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TwxZkx80G6bO"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# ÌõàÎ†® Îß§Í∞úÎ≥ÄÏàò\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtwIo5a0D6f1"
   },
   "source": [
    "### ÌõàÎ†®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "308966d20f264b92a82232b3ec486767",
      "94504f4a2d314a42bcc6ff9746223aae",
      "a7bc68d82a824fcbb94ebe3c2deab9eb",
      "4148ab2d45af493683d9c2aef53482d2",
      "792422a71f2b4b5587e4cd6eef4c95dd",
      "7f89508d047e4c90a63c517a69eaba35",
      "37c72590b0c04bf5b27577a96fdfae3d",
      "2a6ea7ce53e24bdea3099f40ee938d22",
      "34d1b6fa8c994a6d8f26b5fb87a1e6cb",
      "707a969c39b640eb9d79c3eaa7ee2805",
      "2c33fc94d6904c1fb72d88d6a217135e",
      "b5f0653a55494cb1a15709ffa0403168",
      "c11ff8ff5cae4ee0b0c26f791fbe4b46",
      "cb496f5b316a4dafb9bc7c1cda96dc6c",
      "d25b291861134a82b0230201815acd7e",
      "98e30cabfeaa4cb5aea7964ce8712312",
      "d443ba2ef3a644d289515147f202104f",
      "a8ec6f970a014f5785d9dc308d5f2014",
      "074fc8aa5f914a17bab5deff648e1d12",
      "9252ae9c8d314a1184ed5f110ad2669f",
      "3ab261fa79dd41a0b64799b7af01a501",
      "d188545af80349d5ae7da16ff96128bb",
      "6a3cf7d9b7a0443a9891a40828f6079b",
      "c15b0a875f564a5e888e4301a8b6db13",
      "9f554e98b4d745a88008711a698a1c70",
      "47d39f9da03a40c9ace3798e1c76fecb",
      "2259b727321d41929e97aec59cc031fd",
      "d310e01fe50342499523ebd29b92fcbc",
      "fca43edc5f9145c482afee5e623d80ba",
      "d9bb4a6d35b8438fbf863a1823f61b2c",
      "56f855a3f9d64728b7bc2fefcb145d17",
      "44ad6a262f48476aa5e35a2f6fd924cc",
      "2390d89e5b5c4c77829db673c05d2254",
      "86fcb6c4d7d94a07b82ddbdc48c7c5d5",
      "6e1f5d79ed664b35a89821aa1099001d",
      "1684a78292344d6e9ccbc58f630ee5c7",
      "febd3e07d90c482393a720e747138406",
      "d8b67d3bac4343caa46362cd4b30a3c6",
      "da6dbf95ed5e4dffaf26184ec71ba166",
      "c7f523120a17466591990cffdd9ffec4",
      "1c7be117e58a42a6adc3a36ee2ea5744",
      "cb184fe0b1764f36ba027c10f957c823",
      "b2498f5425964b1db69f07e2dc8a0e16",
      "6324e7f6dfe8416c92db8159c5166829"
     ]
    },
    "id": "B2D7RVihsE7Z",
    "outputId": "99ef4955-b684-4aa3-b01b-d353e444c91b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308966d20f264b92a82232b3ec486767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f0653a55494cb1a15709ffa0403168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3cf7d9b7a0443a9891a40828f6079b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fcb6c4d7d94a07b82ddbdc48c7c5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mroadhome\u001b[0m (\u001b[33mroadhome-wagak\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250325_023257-l5h9u6zm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/roadhome-wagak/huggingface/runs/l5h9u6zm' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/roadhome-wagak/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/roadhome-wagak/huggingface' target=\"_blank\">https://wandb.ai/roadhome-wagak/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/roadhome-wagak/huggingface/runs/l5h9u6zm' target=\"_blank\">https://wandb.ai/roadhome-wagak/huggingface/runs/l5h9u6zm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 22:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.474700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.412200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.453200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.385700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.446500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# ÏßÄÎèÑ ÎØ∏ÏÑ∏ ÌäúÎãù Îß§Í∞úÎ≥ÄÏàò ÏßÄÏ†ï\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "\n",
    "    # ÏùºÎ∞òÏ†ÅÏù∏ SFTÏóêÏÑúÎäî Îã§ÏùåÏùÑ ÏÇ≠Ï†úÌïòÏÑ∏Ïöî.\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Î™®Îç∏ ÌõàÎ†®\n",
    "trainer.train()\n",
    "\n",
    "# QLoRA Í∞ÄÏ§ëÏπò Ï†ÄÏû•\n",
    "trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsIBfv1PsId-"
   },
   "source": [
    "### Ïñ¥ÎåëÌÑ∞ Î≥ëÌï©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M6cPdde4Z-ks"
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRAÏôÄ Î≤†Ïù¥Ïä§ Î™®Îç∏ÏùÑ Î≥ëÌï©Ìï©ÎãàÎã§.\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPRYGimIsM2-"
   },
   "source": [
    "### Ï∂îÎ°†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15dJC3ZrdVnK",
    "outputId": "7a357b43-3be0-4c63-c149-5a512e9ea931"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) that can generate human-like language. They are trained on large amounts of data, including text, audio, and video, and are capable of generating complex sentences and phrases that are often difficult to create by humans.\n",
      "\n",
      "LLMs are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots. They can be used to generate text in different languages, such as English, French, or German, and can also be used to generate images, videos, or other forms of content.\n",
      "\n",
      "One of the most significant applications of LLMs is in the field of natural language generation (NLG). NLG is the process of generating human-like language from a computer program. LLMs can be used to generate text in a variety of fields, including marketing, sales, and customer service.\n",
      "\n",
      "Another application of LLMs is in the field of natural language understanding (NLU). NLU is the process of analyzing human-like language and understanding what it means. LLMs can be used to generate human-like language that is understandable by humans.\n",
      "\n",
      "Overall, LLMs are a powerful tool that can be used in a variety of applications to generate human-like language. They are becoming increasingly popular in the field of AI, and their use is expected to continue to grow in the future.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ÏÇ¨Ï†ÑÏóê Ï†ïÏùòÎêú ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Ïù∏Ïä§Ìä∏Îü≠ÏÖò ÌäúÎãùÎêú Î™®Îç∏ÏùÑ Ïã§ÌñâÌï©ÎãàÎã§.\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JNfYZe9vCb8"
   },
   "source": [
    "## ÏÑ†Ìò∏ÎèÑ ÌäúÎãù (PPO/DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar2h9kZ9qmEG"
   },
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "referenced_widgets": [
      "652b0daed88f46158e1552ac3839e9f4",
      "89fa1f9a67d64c81b0013d8b5361846c",
      "eee02f49e92243269835076988708721",
      "1c8361074a9f4cb5923f8c0eb279d067",
      "596d4f316e28466cbe8461fd4cd9589d",
      "60a917763aab47339f40e680b1a471eb",
      "6f8e3ba50f9743c2b834a6fb2dfd5a22",
      "2da71e5c4ccb4a55a9643a5fe939d1de",
      "d9f4479aac09425ba9fb571f4ef13039",
      "a91a001137144e3180f0ac0dff7d8aad",
      "8f73142fe1ea4d6fb03b550de4b5bd1e",
      "e7ab149a418b40a7b87f5613d8a6b4e1",
      "886e7745643e4c76ab68c1f63e2a1f7f",
      "5522d9e32a044fc5940067da5b8adc09",
      "410f27cf29974d389a75acb9ecf2ffe4",
      "ac8d689e163c4ca6ba295b79c0bfc002",
      "9b093fbe496f4c6da49f0e3c0c6b900f",
      "79f8a9d252b14041a2cc7cccdb446bc7",
      "78c3e5ba81a14a19bd516893d25b555f",
      "c161f4b311694d499bf98f0b8da307f9",
      "0ddc740bb60b445caf78bbcdd893558c",
      "8d332b25f6164ca3986334c48f90954e",
      "92927cffaf054db2b9795830cd0acf3f",
      "2da4317ab7e041b19df5707f33fbbdba",
      "75b8683d19b04eafa29e3e9c713915f8",
      "ed02d05867004fcd8a6c5df7bf48b402",
      "c990276634084b74a237d34c6f6f0f7c",
      "dced2e06185741e68086989b812c171f",
      "246c61d783384b3084f1871ef6c007eb",
      "249356b34ef64d9fba7f6493056f9dbc",
      "ce7ec2e4a848457dac2159681fa6f5af",
      "6fe702649b1a489eafcd653a96b4d68e",
      "b0946bb3cae8413e8f6e0acaf82213b5",
      "c1ed21192f0748b2a3a923404a13392c",
      "473c32e0713c4e139e08136dba1e92be",
      "6947ed43251d49909c665e29a1d5d79e",
      "3cf4f482cd2c4fa3893d4a2630bc3b76",
      "587dff3c9ae9410fbe8d91827cfe75a9",
      "d9c24fc998aa4105bd648be5c00888e3",
      "edf97cbefb1e4e948e204320d88c1199",
      "0287475b3e9f4425a349d967a1a30f57",
      "bbb3af836413438893a30df9243941a9",
      "a777e068feff4f75a289a7e00a04546e",
      "754adbac7a794985bc6192286943ac9c",
      "97c512471a924d64b634ab5c96cb537f",
      "f1e9f4758f7f45e4aaa4dd1044b95ee4",
      "c1db4429ea4f41cf8351aa0fd4a836dc",
      "87b93dce707649618e2d87bdaec96e9b",
      "75df777df7e149a885a6a3d5edf22efc",
      "f1d7c7ea1f3544c4bf34a210c4e7ec6a",
      "491d59754c9243208c17f2b0f89b9961",
      "cec3aa1286424db185c336ef9e5a72a5",
      "84a18efbb0174d4f8e9162afa87d8d42",
      "bde795ae08444d6badcbe012136f7b1d",
      "a3fcdf9386af4bd1910342b7cdf54878"
     ]
    },
    "id": "UlbPVO_aac33",
    "outputId": "0f365892-d6ea-4b58-d36f-ed22b7311796"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652b0daed88f46158e1552ac3839e9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ab149a418b40a7b87f5613d8a6b4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/79.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92927cffaf054db2b9795830cd0acf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ed21192f0748b2a3a923404a13392c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c512471a924d64b634ab5c96cb537f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 5922\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"TinyLlamaÏùò <|user|> ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©Ìï¥ ÌîÑÎ°¨ÌîÑÌä∏Î•º Íµ¨ÏÑ±Ìï©ÎãàÎã§\"\"\"\n",
    "\n",
    "    # ÌÖúÌîåÎ¶ø Ìè¨Îß∑ÌåÖ\n",
    "    system = \"<|system|>\\n\" + example['system'] + \"</s>\\n\"\n",
    "    prompt = \"<|user|>\\n\" + example['input'] + \"</s>\\n<|assistant|>\\n\"\n",
    "    chosen = example['chosen'] + \"</s>\\n\"\n",
    "    rejected = example['rejected'] + \"</s>\\n\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÌÖúÌîåÎ¶øÏùÑ Ï†ÅÏö©ÌïòÍ≥† ÎπÑÍµêÏ†Å ÏßßÏùÄ ÎåÄÎãµÏùÑ ÏÑ†ÌÉùÌï©ÎãàÎã§\n",
    "dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "dpo_dataset = dpo_dataset.filter(\n",
    "    lambda r:\n",
    "        r[\"status\"] != \"tie\" and\n",
    "        r[\"chosen_score\"] >= 8 and\n",
    "        not r[\"in_gsm8k_train\"]\n",
    ")\n",
    "dpo_dataset = dpo_dataset.map(format_prompt, remove_columns=dpo_dataset.column_names)\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkCJ4CO5sQG6"
   },
   "source": [
    "### Î™®Îç∏ ÏñëÏûêÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YMmilm7c1-P",
    "outputId": "b3bc534b-a39b-4250-e38a-8147dc51de91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "# 4-ÎπÑÌä∏ ÏñëÏûêÌôî ÏÑ§Ï†ï - QLoRAÏùò Q Îã®Í≥Ñ\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-ÎπÑÌä∏ Ï†ïÎ∞ÄÎèÑ Î™®Îç∏ Î°úÎìú\n",
    "    bnb_4bit_quant_type=\"nf4\",  # ÏñëÏûêÌôî Ï¢ÖÎ•ò\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Í≥ÑÏÇ∞ dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Ïù¥Ï§ë ÏñëÏûêÌôî Ï†ÅÏö©\n",
    ")\n",
    "\n",
    "# LoRAÏôÄ Î≤†Ïù¥Ïä§ Î™®Îç∏ÏùÑ Ìï©Ïπ©ÎãàÎã§.\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# LLaMA ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º Î°úÎìúÌï©ÎãàÎã§.\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iidCbaXMs1O4"
   },
   "source": [
    "### ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "m6IfkvLkylVD"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA ÏÑ§Ï†ï\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Ïä§ÏºÄÏùºÎßÅ\n",
    "    lora_dropout=0.1,  # LoRA Ï∏µÏùò ÎìúÎ°≠ÏïÑÏõÉ\n",
    "    r=16,  # Îû≠ÌÅ¨\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # ÎåÄÏÉÅ Ï∏µ\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# ÌõàÎ†®ÏùÑ ÏúÑÌï¥ Î™®Îç∏ÏùÑ Ï§ÄÎπÑÌï©ÎãàÎã§.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lk-cEEd8nk27"
   },
   "outputs": [],
   "source": [
    "from trl import DPOConfig\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# ÌõàÎ†® Îß§Í∞úÎ≥ÄÏàò\n",
    "training_arguments = DPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=200,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958,
     "referenced_widgets": [
      "f4a0051a8c464f20a920fbee35938289",
      "b1f38923f0e04b40a96bf9321b5034b8",
      "098a572264fe4535b45f6fe2bd9c4ce0",
      "2edbadbc2feb4e76b390ea56bd9ea583",
      "ba44c0c8301247cf8143bd3057c6753d",
      "00950a950b0b4c4a8f5a18962eca0af7",
      "baefbde4f18f44118410a0b89c83373d",
      "24f57a5a5ece4b01a26caad975dd3919",
      "33436d49238943beaa3d4266baec58d0",
      "e78860562d0e4543858fdc312a3e3464",
      "87b3261be7c54ff499e7a38efe2450d2",
      "54cffd460e684b36873960b87a401e8e",
      "27dce52244ec41ae89d8ad20d384c611",
      "f6e4557b766f42de9024cc83a85ee172",
      "d7c0dba09a9f47bd8b7e1cd4471542f5",
      "8d533af3ac2f43d89724db94b74d6ac9",
      "006501aa64d147b7b437aad5cae0ee23",
      "935f7b81190045058df0a16a5b6c074f",
      "3b0bb64176a8458587174ea177d9d529",
      "b56875f9e6324388b34ca545c9af54cc",
      "4d08049ebbe84505bfa53fa49dba1ec2",
      "77ef69aaf40d433e8005a373e72305b1",
      "d292c858bd6f439193272578ae582eb9",
      "608d1fb78119428aafcac20a9923d9fb",
      "a1e068a651a64f08b6fa22a973f4e97b",
      "46c78a6ee6134265a23b833eb4c5f767",
      "a2a18ed66ccd4e7a96ca481654dcba70",
      "ec18e16a478b4b4eae08c23cf1a89477",
      "c7dd8068e341454ba2ec47aa5b4579e5",
      "3195c2bd77254bf8aef3efaa7748ece1",
      "4d5b012ff60f4fbc9fb5948ba666fec3",
      "ecb42024ee90466193e593691f351ba3",
      "4452e99c1dd54ae28e068f96bd3b7b7f"
     ]
    },
    "id": "Pp3tUXhWm0pE",
    "outputId": "f24e4f30-8b21-4934-d0f7-659175cbce6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a0051a8c464f20a920fbee35938289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cffd460e684b36873960b87a401e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d292c858bd6f439193272578ae582eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 23:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.453600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.605800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.557300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.488200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "# DPOTrainer Í∞ùÏ≤¥Î•º ÎßåÎì≠ÎãàÎã§.\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dpo_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "# DPOÎ°ú Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ ÌäúÎãùÌï©ÎãàÎã§.\n",
    "dpo_trainer.train()\n",
    "\n",
    "# Ïñ¥ÎåëÌÑ∞Î•º Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "dpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFE4OKFvyLMe",
    "outputId": "00b2c764-c681-4e4c-f2ac-9a1089463e06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# LoRAÏôÄ Î≤†Ïù¥Ïä§ Î™®Îç∏ÏùÑ Ìï©Ïπ©ÎãàÎã§.\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sft_model = model.merge_and_unload()\n",
    "\n",
    "# DPO LoRAÏôÄ SFT Î™®Îç∏ÏùÑ Ìï©Ïπ©ÎãàÎã§.\n",
    "dpo_model = PeftModel.from_pretrained(\n",
    "    sft_model,\n",
    "    \"TinyLlama-1.1B-dpo-qlora\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "dpo_model = dpo_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAkwJcHYmxr4",
    "outputId": "b467ab94-ef6e-4557-ba6b-cf22597528c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) that can generate human-like language. They are trained on large amounts of data, including text, audio, and video, and are capable of generating complex sentences and phrases that are often difficult to create by humans.\n",
      "\n",
      "LLMs are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots. They can be used to generate text in different languages, such as English, French, or German, and can also be used to generate images, videos, or other forms of content.\n",
      "\n",
      "One of the most significant applications of LLMs is in the field of natural language generation (NLG). NLG is the process of generating human-like language from a computer program. LLMs can be used to generate text in a variety of fields, including marketing, sales, and customer service.\n",
      "\n",
      "Another application of LLMs is in the field of natural language understanding (NLU). NLU is the process of analyzing human-like language and understanding what it means. LLMs can be used to generate human-like language that is understandable by humans.\n",
      "\n",
      "Overall, LLMs are a powerful tool that can be used in a variety of applications to generate human-like language. They are becoming increasingly popular in the field of AI, and their use is expected to continue to grow in the future.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ÏÇ¨Ï†ÑÏóê Ï†ïÏùòÎêú ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Ïù∏Ïä§Ìä∏Îü≠ÏÖò ÌäúÎãùÎêú Î™®Îç∏ÏùÑ Ïã§ÌñâÌï©ÎãàÎã§.\n",
    "pipe = pipeline(task=\"text-generation\", model=dpo_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
